{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Multiple Linear Regression in Statsmodels - Lab"]},{"cell_type":"markdown","metadata":{},"source":["## Introduction\n","In this lab, you'll practice fitting a multiple linear regression model on our Boston Housing Data set!"]},{"cell_type":"markdown","metadata":{},"source":["## Objectives\n","You will be able to:\n","* Run linear regression on Boston Housing dataset with all the predictors\n","* Interpret the parameters of the multiple linear regression model"]},{"cell_type":"markdown","metadata":{},"source":["## The Boston Housing Data"]},{"cell_type":"markdown","metadata":{},"source":["We pre-processed the Boston Housing Data again. This time, however, we did things slightly different:\n","- We dropped \"ZN\" and \"NOX\" completely\n","- We categorized \"RAD\" in 3 bins and \"TAX\" in 4 bins\n","- We transformed \"RAD\" and \"TAX\" to dummy variables and dropped the first variable.\n","- We used min-max-scaling on \"B\", \"CRIM\" and \"DIS\" (and logtransformed all of them first, except \"B\")\n","- We used standardization on \"AGE\", \"INDUS\", \"LSTAT\" and \"PTRATIO\" (and logtransformed all of them first, except for \"AGE\") "]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"['TAX_270_360', 'TAX_360_712']\n['RAD_6_24']\n"}],"source":["import pandas as pd\n","import numpy as np\n","from sklearn.datasets import load_boston\n","boston = load_boston()\n","\n","boston_features = pd.DataFrame(boston.data, columns = boston.feature_names)\n","boston_features = boston_features.drop([\"NOX\",\"ZN\"],axis=1)\n","\n","# first, create bins for based on the values observed. 3 values will result in 2 bins\n","bins = [0, 6,  24]\n","bins_rad = pd.cut(boston_features['RAD'], bins)\n","bins_rad = bins_rad.cat.as_unordered()\n","\n","# first, create bins for based on the values observed. 4 values will result in 3 bins\n","bins = [0, 270, 360, 712]\n","bins_tax = pd.cut(boston_features['TAX'], bins)\n","bins_tax = bins_tax.cat.as_unordered()\n","\n","tax_dummy = pd.get_dummies(bins_tax, prefix=\"TAX\", drop_first=True).rename(columns=lambda col: col.replace(\" \", \"\").replace(\",\", \"_\").replace(\"(\", \"\").replace(\"]\", \"\"))\n","print(tax_dummy.columns.tolist())\n","rad_dummy = pd.get_dummies(bins_rad, prefix=\"RAD\", drop_first=True).rename(columns=lambda col: col.replace(\" \", \"\").replace(\",\", \"_\").replace(\"(\", \"\").replace(\"]\", \"\"))\n","print(rad_dummy.columns.tolist())\n","boston_features = boston_features.drop([\"RAD\",\"TAX\"], axis=1)\n","boston_features = pd.concat([boston_features, rad_dummy, tax_dummy], axis=1)"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["age = boston_features[\"AGE\"]\n","b = boston_features[\"B\"]\n","logcrim = np.log(boston_features[\"CRIM\"])\n","logdis = np.log(boston_features[\"DIS\"])\n","logindus = np.log(boston_features[\"INDUS\"])\n","loglstat = np.log(boston_features[\"LSTAT\"])\n","logptratio = np.log(boston_features[\"PTRATIO\"])\n","\n","# minmax scaling\n","boston_features[\"B\"] = (b-min(b))/(max(b)-min(b))\n","boston_features[\"CRIM\"] = (logcrim-min(logcrim))/(max(logcrim)-min(logcrim))\n","boston_features[\"DIS\"] = (logdis-min(logdis))/(max(logdis)-min(logdis))\n","\n","#standardization\n","boston_features[\"AGE\"] = (age-np.mean(age))/np.sqrt(np.var(age))\n","boston_features[\"INDUS\"] = (logindus-np.mean(logindus))/np.sqrt(np.var(logindus))\n","boston_features[\"LSTAT\"] = (loglstat-np.mean(loglstat))/np.sqrt(np.var(loglstat))\n","boston_features[\"PTRATIO\"] = (logptratio-np.mean(logptratio))/(np.sqrt(np.var(logptratio)))"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>CRIM</th>\n      <th>INDUS</th>\n      <th>CHAS</th>\n      <th>RM</th>\n      <th>AGE</th>\n      <th>DIS</th>\n      <th>PTRATIO</th>\n      <th>B</th>\n      <th>LSTAT</th>\n      <th>RAD_6_24</th>\n      <th>TAX_270_360</th>\n      <th>TAX_360_712</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>-1.704344</td>\n      <td>0.0</td>\n      <td>6.575</td>\n      <td>-0.120013</td>\n      <td>0.542096</td>\n      <td>-1.443977</td>\n      <td>1.000000</td>\n      <td>-1.275260</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>0.153211</td>\n      <td>-0.263239</td>\n      <td>0.0</td>\n      <td>6.421</td>\n      <td>0.367166</td>\n      <td>0.623954</td>\n      <td>-0.230278</td>\n      <td>1.000000</td>\n      <td>-0.263711</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.153134</td>\n      <td>-0.263239</td>\n      <td>0.0</td>\n      <td>7.185</td>\n      <td>-0.265812</td>\n      <td>0.623954</td>\n      <td>-0.230278</td>\n      <td>0.989737</td>\n      <td>-1.627858</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.171005</td>\n      <td>-1.778965</td>\n      <td>0.0</td>\n      <td>6.998</td>\n      <td>-0.809889</td>\n      <td>0.707895</td>\n      <td>0.165279</td>\n      <td>0.994276</td>\n      <td>-2.153192</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.250315</td>\n      <td>-1.778965</td>\n      <td>0.0</td>\n      <td>7.147</td>\n      <td>-0.511180</td>\n      <td>0.707895</td>\n      <td>0.165279</td>\n      <td>1.000000</td>\n      <td>-1.162114</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>","text/plain":"       CRIM     INDUS  CHAS     RM       AGE       DIS   PTRATIO         B  \\\n0  0.000000 -1.704344   0.0  6.575 -0.120013  0.542096 -1.443977  1.000000   \n1  0.153211 -0.263239   0.0  6.421  0.367166  0.623954 -0.230278  1.000000   \n2  0.153134 -0.263239   0.0  7.185 -0.265812  0.623954 -0.230278  0.989737   \n3  0.171005 -1.778965   0.0  6.998 -0.809889  0.707895  0.165279  0.994276   \n4  0.250315 -1.778965   0.0  7.147 -0.511180  0.707895  0.165279  1.000000   \n\n      LSTAT  RAD_6_24  TAX_270_360  TAX_360_712  \n0 -1.275260         0            1            0  \n1 -0.263711         0            0            0  \n2 -1.627858         0            0            0  \n3 -2.153192         0            0            0  \n4 -1.162114         0            0            0  "},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["boston_features.head()"]},{"cell_type":"markdown","metadata":{},"source":["## Run an linear model in Statsmodels"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"OLS Regression Results                            \n==============================================================================\nDep. Variable:                   MEDV   R-squared:                       0.779\nModel:                            OLS   Adj. R-squared:                  0.774\nMethod:                 Least Squares   F-statistic:                     144.9\nDate:                Mon, 28 Oct 2019   Prob (F-statistic):          5.08e-153\nTime:                        16:52:21   Log-Likelihood:                -1458.2\nNo. Observations:                 506   AIC:                             2942.\nDf Residuals:                     493   BIC:                             2997.\nDf Model:                          12                                         \nCovariance Type:            nonrobust                                         \n===============================================================================\n                  coef    std err          t      P>|t|      [0.025      0.975]\n-------------------------------------------------------------------------------\nIntercept       8.6442      3.189      2.711      0.007       2.379      14.910\nCRIM           -1.9538      2.115     -0.924      0.356      -6.110       2.202\nINDUS          -0.8046      0.362     -2.220      0.027      -1.517      -0.093\nCHAS            2.5959      0.796      3.260      0.001       1.032       4.160\nRM              2.6466      0.408      6.488      0.000       1.845       3.448\nAGE             0.0794      0.352      0.226      0.821      -0.612       0.770\nDIS           -10.0962      1.856     -5.439      0.000     -13.743      -6.449\nPTRATIO        -1.4867      0.241     -6.160      0.000      -1.961      -1.013\nB               3.8412      0.986      3.897      0.000       1.905       5.778\nLSTAT          -5.6288      0.354    -15.912      0.000      -6.324      -4.934\nRAD_6_24        1.3380      0.672      1.990      0.047       0.017       2.659\nTAX_270_360    -1.2598      0.600     -2.100      0.036      -2.438      -0.081\nTAX_360_712    -2.1461      0.704     -3.047      0.002      -3.530      -0.762\n==============================================================================\nOmnibus:                      106.730   Durbin-Watson:                   1.093\nProb(Omnibus):                  0.000   Jarque-Bera (JB):              432.101\nSkew:                           0.891   Prob(JB):                     1.48e-94\nKurtosis:                       7.162   Cond. No.                         117.\n==============================================================================\n\nWarnings:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"}],"source":["# Your code here\n","from statsmodels.formula.api import ols\n","from sklearn.linear_model import LinearRegression\n","\n","def lin_reg_summary(df, target, pred, t = 0):\n","    predictors = df.drop(target, axis=1)\n","    if t == 0:\n","        f = target + '~' + \"+\".join(predictors.columns)\n","        model_fit_results = ols(formula=f, data=boston_features).fit()\n","        print(model_fit_results.summary())\n","        return model_fit_results\n","    else:\n","        model_fit_results = LinearRegression().fit(predictors, df[target])\n","        print(\"sklearn Regression Results \")\n","        print(\"==============================================================================\")  \n","        print(\"Dep. Variable:\\t\\t{}\".format(target))      \n","        print(\"Model:\\t\\t\\t{}\".format(model_fit_results))\n","        print(\"==============================================================================\")\n","        print(\"Intercept:\\t\\t{}\".format(model_fit_results.intercept_))\n","        for i in range(0, len(predictors.columns.tolist())):\n","            print(\"{}:\\t\\t\\t{}\".format(predictors.columns[i], model_fit_results.coef_[i]))\n","        print(\"==============================================================================\")\n","        return model_fit_results\n","\n","target = 'MEDV'\n","\n","# we need the target\n","boston_target = pd.DataFrame(data=boston.target, columns=[target])\n","#print(boston_target, \"\\n\")\n","boston_features = pd.concat([boston_features, boston_target], axis=1, join='inner')\n","#print(boston_features.head(), \"\\n\")\n","\n","pred = boston_features.columns.tolist()\n","del pred[-1] # remove 'MEDV' since it is the target\n","\n","mfr_ols = lin_reg_summary(boston_features, target, pred, t=0)"]},{"cell_type":"markdown","metadata":{},"source":["## Run the same model in Scikit-learn"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"sklearn Regression Results \n==============================================================================\nDep. Variable:\t\tMEDV\nModel:\t\t\tLinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)\n==============================================================================\nIntercept:\t\t8.644156137983725\nCRIM:\t\t\t-1.953802330561805\nINDUS:\t\t\t-0.8045754861251412\nCHAS:\t\t\t2.595867762517814\nRM:\t\t\t2.6465711106317364\nAGE:\t\t\t0.07939726661293223\nDIS:\t\t\t-10.096184652632433\nPTRATIO:\t\t\t-1.486665988772452\nB:\t\t\t3.841213903133081\nLSTAT:\t\t\t-5.628793689508368\nRAD_6_24:\t\t\t1.3379631728800436\nTAX_270_360:\t\t\t-1.2597761198832762\nTAX_360_712:\t\t\t-2.146061878770741\n==============================================================================\n"}],"source":["# Your code here - Check that the coefficients and intercept are the same as those from Statsmodels\n","mfr_skl = lin_reg_summary(boston_features, target, pred, t=1)"]},{"cell_type":"markdown","metadata":{},"source":["## Interpret the coefficients for PTRATIO, PTRATIO, LSTAT"]},{"cell_type":"markdown","metadata":{},"source":["- CRIM: per capita crime rate by town\n","- INDUS: proportion of non-retail business acres per town\n","- CHAS: Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n","- RM: average number of rooms per dwelling\n","- AGE: proportion of owner-occupied units built prior to 1940\n","- DIS: weighted distances to five Boston employment centres\n","- RAD: index of accessibility to radial highways\n","- TAX: full-value property-tax rate per $10,000\n","- PTRATIO: pupil-teacher ratio by town\n","- B: 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n","- LSTAT: % lower status of the population"]},{"cell_type":"markdown","metadata":{},"source":["## Predict the house price given the following characteristics (before manipulation!!)\n","\n","Make sure to transform your variables as needed!\n","\n","- CRIM: 0.15\n","- INDUS: 6.07\n","- CHAS: 1        \n","- RM:  6.1\n","- AGE: 33.2\n","- DIS: 7.6\n","- PTRATIO: 17\n","- B: 383\n","- LSTAT: 10.87\n","- RAD: 8\n","- TAX: 284"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["# define min_max and standardize functions to make this easier/cleaner\n","def min_max(df, feature, feature_instance, transform = None):\n","    t_feat_inst = feature_instance if transform is None else transform(feature_instance)\n","    t_max_df_feat = max(df[feature] if transform is None else transform(df[feature]))\n","    t_min_df_feat = min(df[feature] if transform is None else transform(df[feature]))\n","    return (t_feat_inst - t_min_df_feat)/(t_max_df_feat - t_min_df_feat)\n","\n","def standardize(df, feature, feature_instance, transform = None):\n","    t_feat_inst = feature_instance if transform is None else transform(feature_instance)\n","    t_mean_df_feat = np.mean(df[feature] if transform is None else transform(df[feature]))\n","    t_sqrt_var_df_feat = np.sqrt(np.var(df[feature] if transform is None else transform(df[feature])))\n","    return (t_feat_inst - t_mean_df_feat)/t_sqrt_var_df_feat"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["# assumes inputs have NOT been scaled or transformed - this function does that\n","def predict_price(mfr, CRIM, INDUS, CHAS, RM, AGE, DIS, PTRATIO, B, LSTAT, RAD, TAX):\n","    bf2 = pd.DataFrame(boston.data, columns = boston.feature_names)\n","    # We used min-max-scaling on \"B\", \"CRIM\" and \"DIS\" (and logtransformed all of them first, except \"B\")\n","    minmax_B = min_max(bf2, 'B', B)\n","    minmax_CRIM = min_max(bf2, 'CRIM', CRIM, np.log)\n","    minmax_DIS = min_max(bf2, 'DIS', DIS, np.log)\n","    # We used standardization on \"AGE\", \"INDUS\", \"LSTAT\" and \"PTRATIO\" (and logtransformed all of them first, except for \"AGE\")\n","    std_AGE = standardize(bf2, 'AGE', AGE)\n","    std_INDUS = standardize(bf2, 'INDUS', INDUS, np.log)\n","    std_LSTAT = standardize(bf2, 'LSTAT', LSTAT, np.log)\n","    std_PTRATIO = standardize(bf2, 'PTRATIO', PTRATIO, np.log)\n","    bins_rad_2 = pd.cut([RAD], [0, 6, 24])\n","    rad_dummy_2 = pd.get_dummies(bins_rad_2, prefix=\"RAD\", drop_first=True).rename(columns=lambda col: col.replace(\" \", \"\").replace(\",\", \"_\").replace(\"(\", \"\").replace(\"]\", \"\"))\n","    bins_tax_2 = pd.cut([TAX], [0, 270, 360, 712])\n","    tax_dummy_2 = pd.get_dummies(bins_tax_2, prefix=\"TAX\", drop_first=True).rename(columns=lambda col: col.replace(\" \", \"\").replace(\",\", \"_\").replace(\"(\", \"\").replace(\"]\", \"\"))\n","    pred_basis = pd.concat([pd.DataFrame([[minmax_CRIM, std_INDUS, CHAS, RM, std_AGE, minmax_DIS, std_PTRATIO, minmax_B, std_LSTAT]], columns=['CRIM', 'INDUS', 'CHAS', 'RM','AGE', 'DIS', 'PTRATIO', 'B', 'LSTAT']), rad_dummy_2, tax_dummy_2], axis=1, join='inner')\n","    pred_basis\n","    return mfr.predict(pred_basis) # MEDV: median value of owner-occupied homes in $1000s."]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"With values CRIM=6.07, CHAS=1, RM=6.1, AGE=33.2, DIS=7.6, PTRAIO=17, B=383, LSTAT=10.87, RAD=8, and TAX=284, the model predicts the value of the home to be: $23431.98\n"}],"source":["CRIM=0.15\n","INDUS=6.07\n","CHAS=1\n","RM=6.1\n","AGE=33.2\n","DIS=7.6\n","PTRATIO=17\n","B=383\n","LSTAT=10.87\n","RAD=8\n","TAX=284\n","pred_medv = predict_price(mfr_ols, CRIM, INDUS, CHAS, RM, AGE, DIS, PTRATIO, B, LSTAT, RAD, TAX)\n","print(\"With values CRIM={}, CHAS={}, RM={}, AGE={}, DIS={}, PTRAIO={}, B={}, LSTAT={}, RAD={}, and TAX={}, the model predicts the value of the home to be: ${}\".format(INDUS, CHAS, RM, AGE, DIS, PTRATIO, B, LSTAT, RAD, TAX, round(pred_medv[0]*1000, 2)))"]},{"cell_type":"markdown","metadata":{},"source":["## Summary\n","Congratulations! You've fitted your first multiple linear regression model on the Boston Housing Data."]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"}},"nbformat":4,"nbformat_minor":2}